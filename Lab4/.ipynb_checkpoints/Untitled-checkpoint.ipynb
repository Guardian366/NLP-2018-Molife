{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "#from nltk import word_tokenize\n",
    "import numpy as npy\n",
    "import pandas as panda\n",
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "class sentimentClass:\n",
    "    \n",
    "    def __init__(self, normalize=True, classifier = \"logistic\", split_ratio=0.25):\n",
    "        #Initializes the classifier\n",
    "\n",
    "        if classifier == \"logReg\":\n",
    "            self.classifier = LogisticRegression(random_state=1234)\n",
    "        elif classifier == \"NB\":\n",
    "            self.classifier = naive_bayes.MultinomialNB()\n",
    "            \n",
    "        self.normalize = normalize\n",
    "        if self.normalize:\n",
    "            self.vector = TfidfVectorizer(use_idf=True)\n",
    "        else:\n",
    "            #self.vector = TfidfVectorizer(use_idf=True, lowercase = True, stop_words = set(nltk.corpus.stopwords.words('english')))\n",
    "            self.vector = TfidfVectorizer(use_idf=True, lowercase = True, stop_words = set(nltk.corpus.stopwords.words('english')), strip_accents='unicode', tokenizer=tokenize, ngram_range=(1, 2), max_df=0.9, min_df=3, sublinear_tf=True)\n",
    "            \n",
    "    def curate(self, sentence):\n",
    "        #creates tables and vectors which we can fit onto the data \n",
    "        return self.vector.fit_transform(sentence.review)\n",
    "    \n",
    "    \n",
    "    def readFile(self, files):\n",
    "        #Reads all the files and creates one frame for all of them\n",
    "        info = []\n",
    "        X,Y = [], []\n",
    "        for x in files:\n",
    "            strippedInfo = panda.read_csv(x, sep='\\t', names=['review','label'])\n",
    "            data.append(strippedInfo)\n",
    "        info = panda.concat(data)\n",
    "        self.data = info\n",
    "        Y = info.label\n",
    "        self.vector.fit(info.review)\n",
    "        X = self.curate(info)\n",
    "        \n",
    "        return train_test_split(X,Y)\n",
    "    \n",
    "    \n",
    "    def train(self, files):\n",
    "        #trains the classifier using already built in libraries \n",
    "        X_train, X_test, Y_train, Y_test =  self.read(files)                      \n",
    "        self.classifier.fit(X_train,Y_train)\n",
    "        print (X_train.shape,Y_train.shape)     \n",
    "        accuracy = accuracy_score(Y_test,self.classifier.predict_proba(X_test)[:,1])\n",
    "        #prints out the accuracy of the classification\n",
    "        print (\"Accuracy = \",accuracy)\n",
    "        \n",
    "        \n",
    "    def classification(self, sentence):\n",
    "        #Attempts the classification of any sentence parsed to it \n",
    "        classf = panda.read_csv(StringIO(sentence), names=['review'])\n",
    "        X = self.curate(classf)\n",
    "        Y = self.classifier.predict_proba(X)        \n",
    "        return npy.argmax(Y)\n",
    "    \n",
    "    \n",
    "    def classify(self, file):\n",
    "        #classifies sentences within a file and returns a file of classifications denoted by 1 and 0\n",
    "        labels = []\n",
    "        with open(file) as f:\n",
    "            for line in f.readlines():\n",
    "                print(line,self.predict(line))\n",
    "                labels.append(self.predict(line))\n",
    "        \n",
    "        with open('results.txt', 'w') as f:\n",
    "            for label in labels:\n",
    "                f.write(str(label)+\"\\n\")\n",
    "                \n",
    "        print (\"Results from \",file,\" printed to: output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnormalized data, Logistic regression\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0f9e596f81eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m my_clf_ul.train([\"./sentiment_labelled_sentences/amazon_cells_labelled.txt\",\n\u001b[0;32m      4\u001b[0m                   \u001b[1;34m\"./sentiment_labelled_sentences/imdb_labelled.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                   \"./sentiment_labelled_sentences/yelp_labelled.txt\"])\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-96e062b90de2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mTrains\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \"\"\"\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-96e062b90de2>\u001b[0m in \u001b[0;36m_read\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-96e062b90de2>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \"\"\"\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "print (\"Logistic Regression Model with unnormalized sentences\")\n",
    "ul = sentimentClass(normalize=False)\n",
    "ul.train([\"./sentiment_labelled_sentences/amazon_cells_labelled.txt\",\n",
    "                  \"./sentiment_labelled_sentences/imdb_labelled.txt\",\n",
    "                  \"./sentiment_labelled_sentences/yelp_labelled.txt\"])\n",
    "print()\n",
    "\n",
    "print (\"Logistic Rregression with normalized sentences\")\n",
    "nl = sentimentClass(normalize=True)\n",
    "nl.train([\"./sentiment_labelled_sentences/amazon_cells_labelled.txt\",\n",
    "                  \"./sentiment_labelled_sentences/imdb_labelled.txt\",\n",
    "                  \"./sentiment_labelled_sentences/yelp_labelled.txt\"])\n",
    "print()\n",
    "\n",
    "print (\"Naive Bayes with normalized sentences\")\n",
    "un = sentimentClass(normalize=False, clf_type='NB')\n",
    "un.train([\"./sentiment_labelled_sentences/amazon_cells_labelled.txt\",\n",
    "                  \"./sentiment_labelled_sentences/imdb_labelled.txt\",\n",
    "                  \"./sentiment_labelled_sentences/yelp_labelled.txt\"])\n",
    "print()\n",
    "\n",
    "print (\"Naive Bayes with normalized sentences\")\n",
    "nn = sentimentClass(normalize=True, clf_type='NB')\n",
    "nn.train([\"./sentiment_labelled_sentences/amazon_cells_labelled.txt\",\n",
    "                  \"./sentiment_labelled_sentences/imdb_labelled.txt\",\n",
    "                  \"./sentiment_labelled_sentences/yelp_labelled.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_clf_nl.predict(\"This product is really good as fuck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_clf_ul.predict(\"This product is really good as fuck\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
