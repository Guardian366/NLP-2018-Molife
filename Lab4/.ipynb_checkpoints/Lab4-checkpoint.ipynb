{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GUARDIAN\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import numpy as npy\n",
    "import re\n",
    "\n",
    "def readThis(files):\n",
    "    data = []\n",
    "    pos = []\n",
    "    neg = []\n",
    "    with open(files) as f:\n",
    "        for i in f: \n",
    "            data.append(i) \n",
    "            pos.append()\n",
    "            \n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,)\n",
    "features = vectorizer.fit_transform(\n",
    "    data)\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(documents):\n",
    "        \"\"\"\n",
    "        Reads documents into one data frame using panda\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        X,Y = [], []\n",
    "        for document in documents:\n",
    "            xis = pd.read_csv(document, sep='\\t', names=['review','label'])\n",
    "            data.append(xis)\n",
    "        data = pd.concat(data)\n",
    "        self.data = data\n",
    "        Y = data.label\n",
    "        vec.fit(data.review)\n",
    "        X = self.preprocess(data)\n",
    "        \n",
    "        return train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-093f02f97a54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m X_train, X_test, y_train, y_test = train_test_split(df['text'].values, \n\u001b[0m\u001b[0;32m      2\u001b[0m                  \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                  test_size=0.2)\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mre_tok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review'].values, \n",
    "                 df['label'].values,        \n",
    "                 test_size=0.2)\n",
    "\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "vect = CountVectorizer(tokenizer=tokenize)\n",
    "tf_train = vect.fit_transform(X_train)\n",
    "tf_test = vect.transform(X_test)\n",
    "\n",
    "tf_train[i] # feature count vector for training case i\n",
    "y_train[i] # label for training case i\n",
    "\n",
    "p = tf_train[y_train==1].sum(0) + 1\n",
    "q = tf_train[y_train==0].sum(0) + 1\n",
    "r = np.log((p/p.sum()) / (q/q.sum()))\n",
    "b = np.log(len(p) / len(q))\n",
    "pre_preds = tf_test @ r.T + b\n",
    "preds = pre_preds.T > 0\n",
    "accuracy = (preds == y_test).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
